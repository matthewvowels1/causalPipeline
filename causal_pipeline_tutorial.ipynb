{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting 1 CUDA device(s).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from cdt.causality.graph import SAM\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tools.reducer import*\n",
    "from tools.super_learner import*\n",
    "from tools.TLP import TLP\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "from tools.auto_IF import *\n",
    "\n",
    "from tools.bd_SMDAG import *\n",
    "from tools.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Pipeline for Psychologists and Social Scientists\n",
    "\n",
    "This notebook accompanies a paper presenting a causal research pipeline for psychologists and social scientists. The purpose of the notebook is not to detail every possible in a typical research project, but to demonstrate the key steps necessary along the way. It is important to remember, for instance, that deriving a theory about which we are confident can take some back-and-forth, and the output of the causal discovery algorithm is very unlikely to be sufficient on its own. Therefore, in practice, apply the steps in this notebook with caution and dilligence. \n",
    "\n",
    "\n",
    "NOTE: Be careful with variable names. We use 'U' as an indicator of unobserved confounders (e.g. 'U1'), thus if variables contain this symbol you may get errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "\n",
    "Here we import and tidy up the data. We also normalize it (regardless of whether the variables are discrete or continuous) because some causal discovery algorithms incorportating score based objectives have been shown to be highly sensitive to variance (Reisach et al. 2021: https://arxiv.org/abs/2102.13647)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['YRSLIV', 'RAGE', 'RTOTADVRS', 'Rdistress_intensity', 'Rsupport',\n",
      "       'Rsatisfaction', 'RDCI_s_response_r', 'RDCI_r_response_s',\n",
      "       'RDCI_dyadic', 'RDEPSYM', 'SAGE', 'STOTADVRS', 'Sdistress_intensity',\n",
      "       'Ssupport', 'Ssatisfaction', 'SDCI_s_response_r', 'SDCI_r_response_s',\n",
      "       'SDCI_dyadic', 'SDEPSYM', 'GENRELNS_coded', 'RGEND_coded',\n",
      "       'SGEND_coded'],\n",
      "      dtype='object')\n",
      "(419, 22)\n",
      "(403, 22)\n",
      "Index(['Age_R', 'Age_S', 'Gender_R', 'Gender_S', 'Rel_Type', 'Advrs_R',\n",
      "       'Advrs_S', 'Distress_R', 'Support_R', 'Distress_S', 'Support_S',\n",
      "       'R_DCI_S', 'R_DCI_R', 'DCI_Dyd_R', 'S_DCI_S', 'S_DCI_R', 'DCI_Dyd_S',\n",
      "       'Cohab_Len', 'Rel_Sat_R', 'Rel_Sat_S', 'Dep_R', 'Dep_S'],\n",
      "      dtype='object') 22 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Age_R', 'Age_S', 'Gender_R', 'Gender_S', 'Rel_Type', 'Advrs_R',\n",
       "       'Advrs_S', 'Distress_R', 'Support_R', 'Distress_S', 'Support_S',\n",
       "       'R_DCI_S', 'R_DCI_R', 'DCI_Dyd_R', 'S_DCI_S', 'S_DCI_R', 'DCI_Dyd_S',\n",
       "       'Cohab_Len', 'Rel_Sat_R', 'Rel_Sat_S', 'Dep_R', 'Dep_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = 'wide_format_cat_encoded_EPFL.csv'\n",
    "\n",
    "df = pd.read_csv(fn)\n",
    "df = df.drop(['RID', 'SID', 'CID', 'RGEND', 'SGEND', 'GENRELNS'], axis=1) # remove uncoded variables\n",
    "cols = df.columns\n",
    "print(cols)\n",
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "rename_cols = ['Cohab_Len', 'Age_R', 'Advrs_R', 'Distress_R', 'Support_R', 'Rel_Sat_R', 'R_DCI_S', 'R_DCI_R', 'DCI_Dyd_R', \n",
    "               'Dep_R', 'Age_S', 'Advrs_S', 'Distress_S', 'Support_S', 'Rel_Sat_S', 'S_DCI_S', 'S_DCI_R', 'DCI_Dyd_S',\n",
    "                'Dep_S', 'Rel_Type', 'Gender_R', 'Gender_S']\n",
    "df.columns = rename_cols\n",
    "\n",
    "\n",
    "# reorder columns\n",
    "col_order = ['Age_R', 'Age_S', 'Gender_R', 'Gender_S', 'Rel_Type', 'Advrs_R', 'Advrs_S', 'Distress_R', 'Support_R',\n",
    "            'Distress_S', 'Support_S', 'R_DCI_S', 'R_DCI_R', 'DCI_Dyd_R', 'S_DCI_S', 'S_DCI_R', 'DCI_Dyd_S', 'Cohab_Len', \n",
    "            'Rel_Sat_R', 'Rel_Sat_S', 'Dep_R', 'Dep_S']\n",
    "df = df[col_order]\n",
    "\n",
    "cols = df.columns\n",
    "\n",
    "print(cols, len(cols), len(col_order),)\n",
    "\n",
    "# standardize the data for causal discovery\n",
    "df_cd = (df-df.mean()) / df.std()\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a skeleton which constrains the causal discovery\n",
    "\n",
    "Here we use our initial theory to constrain the possible causal structure (e.g. no links backwards in time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]] (22, 22)\n"
     ]
    }
   ],
   "source": [
    "# nothing can cause:\n",
    "no_causes = [0,1,2,3,4,5,6]\n",
    "\n",
    "# constrain causal links\n",
    "skeleton = 1- np.eye((len(cols)),dtype=np.float32)\n",
    "skeleton[:, no_causes] = 0\n",
    "\n",
    "# specify more specific causal link (or rather, the absence thereof)\n",
    "group_cause_A = np.array([11,12,13,14,15,16,17])  \n",
    "group_effect_A = np.array([7,8,9,10])  # group cause A cannot affect group effect A\n",
    "\n",
    "group_cause_B = np.array([18, 19, 20, 21])\n",
    "group_effect_B = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])  # group cause B cannot affect group effect B\n",
    "\n",
    "\n",
    "for cause in group_cause_A:\n",
    "    for effect in group_effect_A:\n",
    "        skeleton[cause, effect] = 0\n",
    "for cause in group_cause_B:\n",
    "    for effect in group_effect_B:\n",
    "        skeleton[cause, effect] = 0\n",
    "        \n",
    "print(skeleton, skeleton.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run causal disc.\n",
    "\n",
    "Here we use SAM (Kalainathan et al. 2020). We take the result averaged over 50 initializations.\n",
    "\n",
    "An alternative approach is given in this markdown cell, too. Note that we have not had good experiences with mutual information (https://github.com/syanga/pycit/blob/master/pycit/estimators/mixed_cmi.py) or the GCIT (https://github.com/alexisbellot/GCIT) approaches. But they are options.\n",
    "\n",
    "```python\n",
    "from pc_alg2 import *\n",
    "\n",
    "alpha = 0.01  # false positive rate\n",
    "knn = 500  # number of nearest neighbours for MI based tests (in general recommend knn /approx (N/10))\n",
    "\n",
    "# Using the modified PC alg originally from pgmpy which takes the additional MI based ci tests\n",
    "# note that mi based tests will take quite some time to run!\n",
    "''' ci_test = {'pearsonr', 'gcit', 'chi_square', 'mixed_mi', 'cont_mi', 'independence_match'}\n",
    "'''\n",
    "\n",
    "# gcit uses https://github.com/alexisbellot/GCIT/blob/master/Tutorial.ipynb\n",
    "c = PC_adapted(df_cd)\n",
    "model = c.estimate(ci_test=\"gcit\", return_type=\"cpdag\", significance_level=alpha, knn=knn)\n",
    "nx.draw(model, with_labels=True, node_color='white', edge_color='k',\n",
    "        node_size=500, font_size=25, arrowsize=20, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [01:03<00:00, 60.18it/s, disc=0.104, gen=-.0616, regul_loss=1.37, tot=-20.7] \n",
      "100%|██████████| 3800/3800 [01:02<00:00, 61.00it/s, disc=-.49, gen=-.0616, regul_loss=2.33, tot=-19.8]   \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 59.23it/s, disc=-.136, gen=-.0619, regul_loss=1.86, tot=-20.4]   \n",
      "100%|██████████| 3800/3800 [01:02<00:00, 60.81it/s, disc=0.069, gen=-.06, regul_loss=2.23, tot=-19.3]    \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 54.36it/s, disc=-.126, gen=-.062, regul_loss=1.91, tot=-20.3]   \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 63.02it/s, disc=-.131, gen=-.0614, regul_loss=2.33, tot=-19.7]  \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 63.22it/s, disc=-.179, gen=-.0621, regul_loss=1.47, tot=-20.8]  \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.40it/s, disc=-.644, gen=-.0608, regul_loss=2.53, tot=-19.3]   \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.42it/s, disc=1.14, gen=-.0438, regul_loss=2.53, tot=-13.2]  \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.44it/s, disc=-.234, gen=-.0612, regul_loss=1.19, tot=-20.7]  \n",
      "100%|██████████| 3800/3800 [01:03<00:00, 60.05it/s, disc=-.285, gen=-.0621, regul_loss=1.89, tot=-20.4]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 59.27it/s, disc=-.306, gen=-.062, regul_loss=2.71, tot=-19.5]  \n",
      "100%|██████████| 3800/3800 [01:03<00:00, 59.96it/s, disc=0.44, gen=-.0557, regul_loss=2.28, tot=-17.7]   \n",
      "100%|██████████| 3800/3800 [01:02<00:00, 61.07it/s, disc=-.242, gen=-.0626, regul_loss=1.69, tot=-20.8]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.93it/s, disc=-.318, gen=-.0629, regul_loss=2.11, tot=-20.4]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 59.16it/s, disc=-.84, gen=-.0587, regul_loss=2.23, tot=-18.8]    \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 59.12it/s, disc=0.126, gen=-.0617, regul_loss=2.26, tot=-19.9] \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 54.52it/s, disc=0.391, gen=-.0386, regul_loss=2.26, tot=-11.6]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.51it/s, disc=-.335, gen=-.0639, regul_loss=2.56, tot=-20.4]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.78it/s, disc=-.0981, gen=-.0612, regul_loss=2.21, tot=-19.8] \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.59it/s, disc=-.201, gen=-.0608, regul_loss=1.79, tot=-20]    \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.81it/s, disc=-.561, gen=-.0588, regul_loss=2.06, tot=-19.1]  \n",
      "100%|██████████| 3800/3800 [01:04<00:00, 58.47it/s, disc=-.217, gen=-.0611, regul_loss=1.56, tot=-20.4]  \n",
      "100%|██████████| 3800/3800 [01:03<00:00, 60.11it/s, disc=-.255, gen=-.0562, regul_loss=2.26, tot=-17.9]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.94it/s, disc=-.527, gen=-.0632, regul_loss=2.33, tot=-20.3]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.73it/s, disc=-.491, gen=-.0592, regul_loss=2.06, tot=-19.2]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.98it/s, disc=0.0922, gen=-.0483, regul_loss=1.81, tot=-15.5] \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.10it/s, disc=0.195, gen=-.0768, regul_loss=2.56, tot=-25]   \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.82it/s, disc=-.409, gen=-.062, regul_loss=2.06, tot=-20.2]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.91it/s, disc=-.393, gen=-.0609, regul_loss=2.51, tot=-19.3]   \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.82it/s, disc=-.0211, gen=-.0617, regul_loss=2.16, tot=-20]   \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.23it/s, disc=-.219, gen=-.0628, regul_loss=2.56, tot=-20]    \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.90it/s, disc=-.158, gen=-.0619, regul_loss=2.16, tot=-20]    \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.82it/s, disc=-.206, gen=-.0621, regul_loss=1.71, tot=-20.6]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.27it/s, disc=-.261, gen=-.062, regul_loss=2.11, tot=-20.1]    \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.16it/s, disc=-.335, gen=-.0614, regul_loss=2.21, tot=-19.8]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.66it/s, disc=-.594, gen=-.0617, regul_loss=2.33, tot=-19.8] \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.46it/s, disc=-.459, gen=-.0608, regul_loss=1.94, tot=-19.9]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.29it/s, disc=-.196, gen=-.0614, regul_loss=2.38, tot=-19.6]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.27it/s, disc=-.158, gen=-.062, regul_loss=2.06, tot=-20.2]   \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.15it/s, disc=-.437, gen=-.063, regul_loss=2.19, tot=-20.4]  \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.47it/s, disc=-.14, gen=-.0616, regul_loss=2.11, tot=-20]     \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.74it/s, disc=4.11, gen=-.0173, regul_loss=2.36, tot=-3.85]   \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.63it/s, disc=-.221, gen=-.0612, regul_loss=2.04, tot=-19.9]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 61.73it/s, disc=-.294, gen=-.0608, regul_loss=2.14, tot=-19.7]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.11it/s, disc=-.128, gen=-.0621, regul_loss=2.43, tot=-19.9]  \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.03it/s, disc=-.235, gen=-.0616, regul_loss=2.23, tot=-19.9] \n",
      "100%|██████████| 3800/3800 [01:01<00:00, 62.09it/s, disc=-.25, gen=-.0606, regul_loss=2.31, tot=-19.4]    \n",
      "100%|██████████| 3800/3800 [01:00<00:00, 62.35it/s, disc=-.0596, gen=-.0621, regul_loss=2.38, tot=-19.9] \n",
      " 72%|███████▏  | 2723/3800 [00:42<00:18, 57.42it/s, disc=-.13, gen=-.0615, regul_loss=2.04, tot=-20]     "
     ]
    }
   ],
   "source": [
    "# Using SAM\n",
    "\n",
    "'''lr (float) – Learning rate of the generators\n",
    "dlr (float) – Learning rate of the discriminator\n",
    "mixed_data (bool) – Experimental – Enable for mixed-type datasets\n",
    "lambda1 (float) – L0 penalization coefficient on the causal filters\n",
    "lambda2 (float) – L2 penalization coefficient on the weights of the neural network\n",
    "nh (int) – Number of hidden units in the generators’ hidden layers (regularized with lambda2)\n",
    "dnh (int) – Number of hidden units in the discriminator’s hidden layers\n",
    "train_epochs (int) – Number of training epochs\n",
    "test_epochs (int) – Number of test epochs (saving and averaging the causal filters)\n",
    "batch_size (int) – Size of the batches to be fed to the SAM model Defaults to full-batch\n",
    "losstype (str) – type of the loss to be used (either ‘fgan’ (default), ‘gan’ or ‘mse’)\n",
    "dagloss (bool) – Activate the DAG with No-TEARS constraint\n",
    "dagstart (float) – Controls when the DAG constraint is to be introduced in the training (float ranging from 0 to 1, 0 denotes the start of the training and 1 the end)\n",
    "dagpenalisation (float) – Initial value of the DAG constraint\n",
    "dagpenalisation_increase (float) – Increase incrementally at each epoch the coefficient of the constraint\n",
    "functional_complexity (str) – Type of functional complexity penalization (choose between ‘l2_norm’ and ‘n_hidden_units’)\n",
    "hlayers (int) – Defines the number of hidden layers in the generators\n",
    "dhlayers (int) – Defines the number of hidden layers in the discriminator\n",
    "sampling_type (str) – Type of sampling used in the structural gates of the model (choose between ‘sigmoid’, ‘sigmoid_proba’ and ‘gumble_proba’)\n",
    "linear (bool) – If true, all generators are set to be linear generators\n",
    "nruns (int) – Number of runs to be made for causal estimation Recommended: >=32 for optimal performance\n",
    "njobs (int) – Numbers of jobs to be run in Parallel Recommended: 1 if no GPU available, 2*number of GPUs else\n",
    "gpus (int) – Number of available GPUs for the algorithm\n",
    "verbose (bool) – verbose mode\n",
    "'''\n",
    "\n",
    "lrs = [0.01]\n",
    "daglosses = [True]\n",
    "dagpenalizations = [0.06]\n",
    "num_runs = 50\n",
    "\n",
    "\n",
    "\n",
    "'''skeleton (numpy.ndarray) – \n",
    "A priori knowledge about the causal relationships as an adjacency matrix. \n",
    "Can be fed either directed or undirected links.\n",
    "'''\n",
    "\n",
    "\n",
    "for lr in lrs:\n",
    "    for dagloss in daglosses:\n",
    "        for dagpenalization in dagpenalizations:\n",
    "            print(dagpenalization)\n",
    "            \n",
    "            settings = 'lr_' + str(lr) + '_dagloss_' + str(dagloss) + '_dagpen_' + str(dagpenalization) + '_numruns_' + str(num_runs)\n",
    "            \n",
    "            obj = SAM(lr=lr, dlr=0.001, mixed_data=True, lambda1=10, lambda2=0.001, nh=20,\n",
    "                       dnh=200, train_epochs=3000, test_epochs=800, batch_size=- 1, losstype='fgan',\n",
    "                       dagloss=dagloss, dagstart=0.5, dagpenalization=0, dagpenalization_increase=dagpenalization, \n",
    "                        hlayers=2, dhlayers=2, sampling_type='sigmoidproba',\n",
    "                       linear=False, nruns=num_runs, njobs=1, gpus=1, verbose=None)\n",
    "\n",
    "            output = obj.predict(df_cd, graph=skeleton.T) \n",
    "            \n",
    "            nx.write_gml(output, \"graphs/SAM_solution_{}.gml\".format(settings))\n",
    "\n",
    "            \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Threshold the discovered structure.\n",
    "\n",
    "SAM gives us an adjacency matrix with continuous 'confidences' between 0 and 1. Thus, we need to decide on a threshold. Looking at the distribution of confidences can give us some idea of where to threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "title ='S'\n",
    "graph_name = 'SAM_solution_lr_0.01_dagloss_True_dagpen_0.05_numruns_50.gml'\n",
    "graph_name_short = '.'.join(graph_name.split('.')[:-1])\n",
    "\n",
    "output = nx.read_gml(\"graphs/\" + graph_name, destringizer=int)\n",
    "\n",
    "threshold = 0.5\n",
    "G_thresh = threshold_graph(output, threshold=threshold)\n",
    "plot_subbgraph(G=G_thresh, variables=cols, subgraph_name=title+graph_name_short + '_all_{}'.format(threshold), size=(16,12), plot_adj=True, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove any cycles in the graph.\n",
    "\n",
    "We plan to use the DAG framework to identify and estimate the causal effect of interest. We therefore need the graph to be acylclic. In practice, some time may be required to remove these links or integrate them into the theory and use a different analytical method. For the purposes of demonstration, we just remove the links as they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glen = 1\n",
    "while glen > 0:\n",
    "    cycle_list = list(nx.simple_cycles(G_thresh))\n",
    "    print(cycle_list)\n",
    "    glen = len(cycle_list)\n",
    "    if glen == 0:\n",
    "        break\n",
    "    else:\n",
    "        G_thresh.remove_edge(cycle_list[0][0], cycle_list[0][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... As it turns out, there were no cycles anyway.\n",
    "\n",
    "\n",
    "\n",
    "### 6. Add unobserved confounders with edge plausibility scores\n",
    "Causal discovery algorithms are not good at inferring unobserved confounders with observational data. Therefore, to improve the plausibility of our discovered graph, we should be conservative and incorporate some unobserved confounders.\n",
    "\n",
    "In particular, we include some confounders between dyadic variables (like relationships satisfaction for each partner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights of 1 to 'observed' edges\n",
    "for u,v,d in G_thresh.edges(data=True):\n",
    "    d['weight'] = 1.0\n",
    "\n",
    "# introduced confounders and probabilities betwen 0.5 and 1 \n",
    "# (probs < 0.5 are less than likely to exist, so are omitted)\n",
    "G_thresh.add_edge('U1','Dep_S', weight=0.6)\n",
    "G_thresh.add_edge('U1','Dep_R', weight=0.6)\n",
    "G_thresh.add_edge('U2','Rel_Sat_S', weight=0.7)\n",
    "G_thresh.add_edge('U2','Rel_Sat_R', weight=0.7)\n",
    "G_thresh.add_edge('U3','Support_R', weight=0.6)\n",
    "G_thresh.add_edge('U3','Support_S', weight=0.6)\n",
    "G_thresh.add_edge('U4','Dep_R', weight=0.55)\n",
    "G_thresh.add_edge('U4','Distress_R', weight=0.55)\n",
    "G_thresh.add_edge('U4','Dep_S', weight=0.55)\n",
    "G_thresh.add_edge('U4','Distress_S', weight=0.55)\n",
    "G_thresh.add_edge('U5','S_DCI_S', weight=0.55)\n",
    "G_thresh.add_edge('U5','S_DCI_R', weight=0.55)\n",
    "G_thresh.add_edge('U6','DCI_Dyd_R', weight=0.55)\n",
    "G_thresh.add_edge('U6','DCI_Dyd_S', weight=0.55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Reduce the graph \n",
    "\n",
    "The graph is evidently quite large, and yet we may only be interested in estimating a very specific effect. This is not a bad thing - we should have a large graph to ensure we get (for example) the variables necessary to at least form a Markov Blanket around the effect(s) we care about. Once we have our graph and our research question, however, there are likely opportunities to reduce the graph to the key variable for estimation (backdoor adjustment variables, the cause and outcomes of interest, and any precision variables).\n",
    "\n",
    "The causal pipeline package has a number of ways to do this. But given that we have edge probabilities, we should start with the backdoor semi markovian reduction algorithms in bd_SMGDAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'Dep_R'\n",
    "cause = 'Distress_R'\n",
    "\n",
    "# TEST BACKDOOR ID already\n",
    "check, bd_set = check_bd_id(graph=G_thresh, X=cause, Y=outcome)\n",
    "\n",
    "if check:\n",
    "    print('Is identifiable with backdoor set:', bd_set)\n",
    "else:\n",
    "    print('Backdoor criterion not fulfilled, finding confounders to remove...')\n",
    "    # RUNNING BRUTE FORCE ALGO TO FIND SOLUTION AND COST\n",
    "    E, best_cost = bd_brute(graph=G_thresh, X=cause, Y=outcome)\n",
    "\n",
    "    if best_cost != np.inf:\n",
    "        print('Identifiable with the removal of:', E, ' at a cost of:', best_cost)\n",
    "    else:\n",
    "        print('No solution.')\n",
    "\n",
    "    if best_cost != 0:\n",
    "        # FINDING PLAUSIBILITY RATIO OF SOLUTION TO ORIGINAL GRAPH\n",
    "        before_logsum, after_logsum, cutset_invlogsum = get_probs(E, G_thresh)\n",
    "        ratio = np.exp((after_logsum + cutset_invlogsum)) / np.exp(before_logsum)\n",
    "        print('Plausibility ratio:', ratio)\n",
    "    else:\n",
    "        ratio = 1.0\n",
    "\n",
    "    # REMOVE NODES FROM GRAPH AND TEST BACKDOOR ID AGAIN, PROVIDING SUFFICIENT ADJUSTMENT SET\n",
    "    for node in E:\n",
    "        G_thresh.remove_node(node)\n",
    "\n",
    "    check, bd_set = check_bd_id(graph=G_thresh, X=cause, Y=outcome)\n",
    "\n",
    "    if check:\n",
    "        print('Is identifiable with backdoor set:', bd_set)\n",
    "    else:\n",
    "        print('Backdoor criterion not fulfilled.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "The results above tell us that U4 was stopping us from achieving backdoor identification, and that after its removal, our graph is given a before-after plausibility ratio of 0.8. One can interpret this as - our new graph is 80% as plausible as the original graph, assuming the original was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 8. Remove remaining unobserved confounders\n",
    "Now that we have removed unobserved confounders which affect our backdoor adjustment, we can also remove any which remain - we cannot do anything with them because they are unobserved!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G_thresh.nodes())\n",
    "\n",
    "for node in nodes:\n",
    "    if 'U' in node:\n",
    "        G_thresh.remove_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 9. Reduce graph and identify backdoor set and precision variables\n",
    "Technically, we already have what we need from step  7 above. But sometimes we can improve estimation if we include precision variables. These are variables which do not help us identify the effect we care about, but which may help improve the precision of our estimate in finite samples. The next step identifies the backdoor variables and precision variables, and it also reduces the graph according to 'projection' - e.g. mediated paths are combined, and the precision variables are removed from the graph. The graph reduction step isn't necessary for our subsequent estimation, but it can be used to provide a minimal graph for purposes of linear SEM modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rd, confs, precs = reducer(G_thresh, [cause], [outcome],  remove_precision=False, project_causes=True, project_confs=True)\n",
    "\n",
    "print('Number of edges before reduction: ', len(list(G_thresh.edges())))\n",
    "print('Number of edges after reduction: ', len(list(rd.edges())))\n",
    "\n",
    "# plot the reduced graph\n",
    "plot_subbgraph(G=rd, variables=cols, subgraph_name=title+graph_name_short + '_reduced_{}'.format(threshold), size=(16,12), plot_adj=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (alternative) Identify the Effect (again... so not  really necessary) \n",
    "\n",
    "We already know the effect is identifiable under the backdoor criterion, but we can demonstrate the use of an alternative approach to general identification, in case it is useful. \n",
    "\n",
    "For this we use the causaleffect package https://github.com/pedemonte96/causaleffect  https://arxiv.org/abs/2107.04632\n",
    "\n",
    "N.B. The function only accepts graph variable/node names without spaces. e.g. 'Dep R' should be 'Dep_R'. verbose=False is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = check_id(rd, cause=cause, effect=outcome, verbose=False)\n",
    "\n",
    "# if one needs to view the formula for identification\n",
    "# display(Math(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Targeted Learning and Inference\n",
    "\n",
    "Let us recap what we have done so far\n",
    "\n",
    "- causal discovery\n",
    "- added unobserved confounders with edge plausibilities\n",
    "- identified the most plausible subgraph that facilitates backdoor adjustment\n",
    "- reduce this subgraph futher to contain the nodes and edges necessary to estimate a target effect\n",
    "- establish a list of backdoor adjustment variables and precision variables\n",
    "\n",
    "Now we are ready to do some estimation. The process can be broken down as follows:\n",
    "\n",
    "1. Specify the outcome min and max - we will transform the outcome into the range 0 to 1, so need the max and mins of the scale! \n",
    "2. Specify the SuperLearner dictionaries for the treatment(G) and outcome (Q) models  See Phillips et al. (2022) for some recommendations https://arxiv.org/abs/2204.06139\n",
    "3. Check list of confounders and precision variables. It so happens that there are no confounders for this choice of treatment and outcome. This is not a problem for tareted learning - we simply compute the marginal probabilities of belonging to a certain treatment group, and the G superlearner is not used.\n",
    "4.  Specify whether the outcome is continuous of categorical (in our case it is continuous)\n",
    "5. Make sure the treatment variable has the reference group as 0. For example, in our data, Distress_R ranges from 1 to 5, so if we want group 1 to be the reference, we subtract one from all datapoints.  Also specify the group comparisons required for the chosen treatment.\n",
    "6. Set the number of folds in k-fold. See Phillips et al. (2022) for some recommendations https://arxiv.org/abs/2204.06139\n",
    "7. Initialise a TLP targeted learning class and...\n",
    "8. run targeted learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - we need the scale min and max bounds so that we can scale the outcome between 0 and 1\n",
    "# in practice set this to the actual scale range (rather than the empirical range)\n",
    "outcome_scale_ranges = [df[outcome].min(), df[outcome].max()]  \n",
    "outcome_min, outcome_max = outcome_scale_ranges[0], outcome_scale_ranges[1]\n",
    "\n",
    "# step 2 - specify the superlearner candidate learners\n",
    "est_dict_Q = ['Elastic', 'BR', 'SV', 'LR', 'RF', 'MLP', 'AB', 'poly']\n",
    "est_dict_G = ['LR', 'NB', 'MLP','SV', 'poly', 'RF','AB']\n",
    "\n",
    "# step 3\n",
    "i = 0\n",
    "print('outcome: ', outcome, '. cause: ', cause, '. Confounders:', confs, '. Precisions:', precs, '\\n')\n",
    "\n",
    "# step 4\n",
    "outcome_type = 'reg'  # 'reg' or 'cls'\n",
    "\n",
    "# step 5\n",
    "df.Distress_R = df.Distress_R - 1  # don't do this multiple times!\n",
    "group_comparisons =[[2,0], [4,0]]  # comparison in list format with 'group B [vs] reference_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6\n",
    "k = 8  # number of folds for SL training\n",
    "\n",
    "# step 7\n",
    "tlp = TLP(df, cause=cause, outcome=outcome, confs=list(confs),\n",
    "          precs=list(precs), outcome_type=outcome_type, Q_learners=est_dict_Q, G_learners=est_dict_G,\n",
    "         outcome_upper_bound=outcome_max, outcome_lower_bound=outcome_min, seed=0)\n",
    "\n",
    "# step 8 \n",
    " # fit SuperLearners. NB standardized_outcome is just for the fitting (this is totally separate to the targeted learning scaling)\n",
    "all_preds_Q, gts_Q, all_preds_G, gts_G = tlp.fit(k=k, standardized_outcome=False, calibrationQ=True, calibrationG=False)\n",
    "\n",
    "# 'do' targeted learning\n",
    "pre_update_effects, post_update_effects, ses, ps = tlp.target_multigroup(group_comparisons=group_comparisons)\n",
    "\n",
    "# 'do' targeted learning with double robust inference\n",
    "# pre_update_effects_dr, post_update_effects_dr, ses_dr, ps_dr = tlp.dr_target_multigroup(group_comparisons=group_comparisons)\n",
    "\n",
    "\n",
    "naive_effects = {}\n",
    "naive_ses = {}\n",
    "naive_ps = {}\n",
    "for group_comparison in group_comparisons:\n",
    "    group_a = group_comparison[0]\n",
    "    group_ref = group_comparison[1]\n",
    "\n",
    "    data_a = (df[df[cause] == group_a][outcome] - outcome_min) / (outcome_max - outcome_min)\n",
    "    data_ref = (df[df[cause] == group_ref][outcome] - outcome_min) / (outcome_max - outcome_min)\n",
    "    na = len(data_a)\n",
    "    nref = len(df)\n",
    "    naive = data_a.mean() - data_ref.mean()\n",
    "    data_a_se = np.std(data_a, ddof=1) / (np.sqrt(na))\n",
    "    data_ref_se = np.std(data_ref, ddof=1) / (np.sqrt(nref))\n",
    "    diff_se = np.sqrt(data_a_se**2 + data_ref_se**2)\n",
    "\n",
    "    upper, lower = naive + 1.96*diff_se, naive - 1.96*diff_se\n",
    "\n",
    "    p = 2 * (1 - t.cdf(np.abs(naive) /diff_se, na+nref))\n",
    "\n",
    "    naive_effects[str(group_comparison)] = naive\n",
    "    naive_ses[str(group_comparison)] = (diff_se, upper, lower)\n",
    "    naive_ps[str(group_comparison)] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Plot the results / perform inference\n",
    "\n",
    "We plot the results below. We can see that the targeting step has increased the precision of the estimates compared with the naive group differences. In this example, there were no confounders for our choice of cause and effect. But one could set the list of confounders to be empty (if it isn't) and re-run it, to get an estimation for how much the confounders are affecting the estimate under deliberate structural misspecification. With this one can create a sensitivity analysis by comparing the results for different amounts of structural misspecificatoin. One can also discount the validity of the results according to the plausibility ratio we derived when establishing the backdoor identifiable subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "est_2_0 = post_update_effects['[2, 0]']\n",
    "err_2_0 = 1.96 * ses['[2, 0]'][0]\n",
    "\n",
    "est_2_0_pre = pre_update_effects['[2, 0]']\n",
    "\n",
    "est_2_0_naive = naive_effects['[2, 0]']\n",
    "err_2_0_naive = 1.96 * naive_ses['[2, 0]'][0]\n",
    "\n",
    "est_4_0 = post_update_effects['[4, 0]']\n",
    "err_4_0 = 1.96 * ses['[4, 0]'][0]\n",
    "\n",
    "est_4_0_pre = pre_update_effects['[4, 0]']\n",
    "\n",
    "est_4_0_naive = naive_effects['[4, 0]']\n",
    "err_4_0_naive = 1.96 * naive_ses['[4, 0]'][0]\n",
    "\n",
    "plt.errorbar(x=[outcome + '  2vs0 naive'], y=est_2_0_naive, yerr=err_2_0_naive, capsize=5, fmt='x', color='teal', label='naive')\n",
    "plt.errorbar(x=[outcome + ' 2vs0 pre'], y=est_2_0_pre, yerr=0, capsize=10, fmt='o', color='r', label='pre-targeted')\n",
    "plt.errorbar(x=[outcome + ' 2vs0 targeted'], y=est_2_0, yerr=err_2_0, capsize=10, fmt='o', color='k', label='targeted')\n",
    "\n",
    "plt.errorbar(x=[outcome + ' 4vs0 naive'], y=est_4_0_naive, yerr=err_4_0_naive, capsize=5, fmt='x', color='teal')\n",
    "plt.errorbar(x=[outcome + ' 4vs0 pre'], y=est_4_0_pre, yerr=0, capsize=10, fmt='o', color='r')\n",
    "plt.errorbar(x=[outcome + ' 4vs0 targeted'], y=est_4_0, yerr=err_4_0, capsize=10, fmt='o', color='k')\n",
    "\n",
    "\n",
    "plt.xlabel('Outcome')   \n",
    "plt.axhline(y = 0, color = 'k', linestyle = '-') \n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylim(-.22, .3)\n",
    "plt.legend()\n",
    "plt.savefig('tl_results/' + outcome+'_targeted_comparison.png', bbox_inches=\"tight\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Sensitivity Analysis\n",
    "\n",
    "Run an analysis for 'worst case' scenario (no adjustment). In the running example, we actually didn't identify any confounders vai causal discovery anyway, and researchers should really investigate this further before continuing (especially for psychology, where there are confounders galore...). However, for the sake of the illustration, we assume that our precision variables were actually miss-allocated confounders, and so for sensitivity analysis, we remove them all and re-run the analysis to understand the impact. This we quantify as $\\delta = 1$, and we can then great a family of results for different multiples of $\\delta$ to understand how much confounding would be necessary in practice to change our inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "k = 8  # number of folds for SL training\n",
    "\n",
    "# step 2 - specify empty confounders and precision sets\n",
    "tlp = TLP(df, cause=cause, outcome=outcome, confs=[],\n",
    "          precs=[], outcome_type=outcome_type, Q_learners=est_dict_Q, G_learners=est_dict_G,\n",
    "         outcome_upper_bound=outcome_max, outcome_lower_bound=outcome_min, seed=0)\n",
    "\n",
    "# step 3\n",
    " # fit SuperLearners\n",
    "all_preds_Q, gts_Q, all_preds_G, gts_G = tlp.fit(k=k, standardized_outcome=False, calibrationQ=True, calibrationG=False)\n",
    "\n",
    "# step 4 'do' targeted learning\n",
    "pre_update_effects_delta, post_update_effects_delta, ses_delta, ps_delta = tlp.target_multigroup(group_comparisons=group_comparisons)\n",
    "\n",
    "\n",
    "\n",
    "# step 5 specify contrast of interest\n",
    "grp = '[4, 0]'\n",
    "\n",
    "# step 6 specify the multipls / values of delta you want to explore \n",
    "multiples = [0.5,1, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "# step 7\n",
    "# get the difference between our intentionally biased estimate and the one for which we assume delta = 0 \n",
    "est_biased = post_update_effects_delta[grp]\n",
    "\n",
    "est_unbiased = post_update_effects[grp]\n",
    "err_unbiased = 1.96 * ses[grp][0]\n",
    "\n",
    "diff_est = np.abs(est_biased - est_unbiased)\n",
    "\n",
    "\n",
    "# step 8 get the plots\n",
    "# plot negative range of multiples of delta \n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for multiple in reversed(multiples):\n",
    "    biased_est_neg = est_unbiased - multiple*diff_est  # get original - amount of bias\n",
    "    plt.errorbar(x=['-{}'.format(multiple)], y=biased_est_neg, yerr=err_unbiased, capsize=10, fmt='o', color='k', label='pre-targeted')\n",
    "\n",
    "# plot original estimate (assuming delta=0) \n",
    "plt.errorbar(x=['0'], y=est_unbiased, yerr=err_unbiased, capsize=10, fmt='o', color='k', label='pre-targeted')\n",
    "plt.axhline(y=0.0, color='r', linestyle='--')\n",
    "plt.axvline(x=5, color='r', linestyle='--')\n",
    "    \n",
    "for multiple in multiples:\n",
    "    biased_est = est_unbiased + multiple*diff_est # get original + amount of bias\n",
    "    \n",
    "    plt.errorbar(x=['{}'.format(multiple)], y=biased_est, yerr=err_unbiased, capsize=10, fmt='o', color='k', label='pre-targeted')\n",
    "\n",
    "plt.ylim(-.22, .7)\n",
    "plt.xlabel('delta')   \n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('tl_results/' + outcome+'_targeted_comparison_sensitivity_4-0.png', bbox_inches=\"tight\", dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
